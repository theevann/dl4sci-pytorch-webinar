{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"https://discuss.pytorch.org/uploads/default/original/2X/3/35226d9fbc661ced1c5d17e374638389178c3176.png\" width=\"400\" style=\"margin: 50px auto; display: block; position: relative; left: -30px;\" />\n",
    "</div>\n",
    "\n",
    "<!--NAVIGATION-->\n",
    "# < [CNN](5-CNN.ipynb) | Transfer Learning |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning\n",
    "\n",
    "Transfer Learning is the re-use of pre-trained models on new tasks. Most often, the two tasks are different but somehow related to each other. For example, a model which was trained on image classification might have learnt image features which can also be harnessed for other image related tasks. This technique became increasingly popular in the field of Deep Learning since it enables one to train a model on comparatively little data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![one does not simply build models](figures/simply-build-model.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "#### 1. [Setup](#Setup)\n",
    "#### 2. [Data loading and pre-processing](#Data-loading-and-pre-processing)\n",
    "#### 3. [Loading a pre-trained model](#Loading-a-pre-trained-model)\n",
    "#### 4. [Training the last layer](#Training-the-last-layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Setup\n",
    "\n",
    "In this notebook, we consider the [Alien vs Preditor](https://www.kaggle.com/pmigdal/alien-vs-predator-images) task from [Kaggle](http://www.kaggle.com). We want to classify images as either 'aliens' or 'predators'. \n",
    "\n",
    "Because the dataset is relatively small, and we don't want to wait for hours, we use a model pre-trained on the very large ImageNet task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Transfer Learning Figure 1](figures/transfer-learning-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce the difficulty of training, we freeze the intermediate layers and only train a few layers close to the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Transfer Learning Figure 2](figures/transfer-learning-2.png)\n",
    "Figures taken from https://www.kaggle.com/pmigdal/alien-vs-predator-images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute this cell to download the alien-vs-predator dataset and to install some dependencies. Google Colab will offer you to restart the kernel after you did this. Please do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: pip: not found\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   388    0   388    0     0    155      0 --:--:--  0:00:02 --:--:--   155\n",
      "100 7247k    0 7247k    0     0  1957k      0 --:--:--  0:00:03 --:--:-- 8320k\n",
      "total 0\n",
      "drwxrwxrwx 1 evann evann 4096 Jan 25  2019 train\n",
      "drwxrwxrwx 1 evann evann 4096 Jan 25  2019 validation\n",
      "/bin/sh: 1: pip: not found\n"
     ]
    }
   ],
   "source": [
    "!wget -q https://raw.githubusercontent.com/theevann/amld-pytorch-workshop/master/binder/requirements.txt -O requirements.txt\n",
    "!pip install -qr requirements.txt\n",
    "\n",
    "!mkdir -p data\n",
    "!curl -L -o alien-vs-predator.zip \"https://drive.google.com/uc?id=1IGiEW3Vtf-ZiLINHCGVDM0NRSkyiYT98&export=download\"\n",
    "!unzip -oq alien-vs-predator.zip -d data/\n",
    "!rm alien-vs-predator.zip\n",
    "!ls -l data/alien-vs-predator/\n",
    "\n",
    "# for PIL.Image\n",
    "!pip install --no-cache-dir -I Pillow==6.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all went well, you should be able to execute the following cell successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchvision\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "matplotlib.rc('font', size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Data loading and pre-processing\n",
    "\n",
    "Here we will create a `Dataset` and corresponding `DataLoader` that find training examples in the following directory structure:\n",
    "\n",
    "```\n",
    "data/alien-vs-predator\n",
    "│\n",
    "├──train\n",
    "│     │\n",
    "│     │───alien\n",
    "│     │    ├── 20.jpg\n",
    "│     │    ├── 104.jpg\n",
    "│     │    ├── ...\n",
    "│     │\n",
    "│     └───predator\n",
    "│          │   1.jpg\n",
    "│          │   78.jpg\n",
    "│          ├── ...\n",
    "│\n",
    "└── validation\n",
    "      ├── alien  \n",
    "      │    │   233.jpg\n",
    "      │    │   12.jpg\n",
    "      │    ├── ...\n",
    "      │\n",
    "      └───predator\n",
    "           │   22.jpg\n",
    "           │   77.jpg\n",
    "           ├── ...\n",
    "```\n",
    "\n",
    "```\n",
    "data/human-vs-dog\n",
    "│\n",
    "├──train\n",
    "│     │\n",
    "│     │───human\n",
    "│     │    ├── 20.jpg\n",
    "│     │    ├── 104.jpg\n",
    "│     │    ├── ...\n",
    "│     │\n",
    "│     └───dog\n",
    "│          │   1.jpg\n",
    "│          │   78.jpg\n",
    "│          ├── ...\n",
    "│\n",
    "└── validation\n",
    "      ├── human  \n",
    "      │    │   233.jpg\n",
    "      │    │   12.jpg\n",
    "      │    ├── ...\n",
    "      │\n",
    "      └───dog\n",
    "           │   22.jpg\n",
    "           │   77.jpg\n",
    "           ├── ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torchvision` datasets allow us to specify many different transformation on the inputs. Random perturbations can improve the quality of your model by synthetically enlarging your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    RandomResizedCrop,\n",
    "    RandomHorizontalFlip,\n",
    "    ToTensor,\n",
    "    Resize,\n",
    "    CenterCrop,\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create datasets\n",
    "train_data = ImageFolder(\n",
    "    os.path.join(os.getcwd(), \"data\", \"alien-vs-predator\", \"train\"),\n",
    "    transform=Compose(\n",
    "        [RandomResizedCrop(224), RandomHorizontalFlip(), ToTensor()]  # data augmentation\n",
    "    ),\n",
    ")\n",
    "\n",
    "test_data = ImageFolder(\n",
    "    os.path.join(os.getcwd(), \"data\", \"alien-vs-predator\", \"validation\"),\n",
    "    transform=Compose([Resize(256), CenterCrop(224), ToTensor()]),  # give images the same size as the train images\n",
    ")\n",
    "\n",
    "# Specify corresponding batched data loaders\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "# Our datasets have two classes:\n",
    "class_names = train_data.classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation\n",
    "\n",
    "Let's have a look at the effect of the transformations we specified for data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    RandomResizedCrop,\n",
    "    RandomHorizontalFlip,\n",
    "    ToTensor,\n",
    "    Resize,\n",
    "    CenterCrop,\n",
    ")\n",
    "\n",
    "# Look at one image from the dataset\n",
    "preview_data = ImageFolder(os.path.join(os.getcwd(), \"data\", \"alien-vs-predator\", \"train\"))\n",
    "img, label = preview_data[2]\n",
    "\n",
    "# Let's inspect the effect of the various transformers\n",
    "fig = plt.figure(figsize=(16, 9))\n",
    "\n",
    "def show_image(img, label):\n",
    "    # permute turns (rgb, height, width) into (height, width, rgb)\n",
    "    plt.imshow(tensor_transformer(img).permute(1, 2, 0))\n",
    "    plt.xlabel(label)\n",
    "\n",
    "tensor_transformer = ToTensor()\n",
    "plt.subplot(1, 4, 1)\n",
    "show_image(img, \"Original\")\n",
    "\n",
    "resize_transformer = Resize(400)\n",
    "plt.subplot(1, 4, 2)\n",
    "show_image(resize_transformer(img), \"Resized (400x400)\")\n",
    "\n",
    "horizontal_flip_transformer = RandomHorizontalFlip()\n",
    "plt.subplot(1, 4, 3)\n",
    "show_image(horizontal_flip_transformer(img), \"Random horizontal flip\")\n",
    "\n",
    "random_resize_crop_transformer = RandomResizedCrop(250, scale=(0.5, 1))\n",
    "plt.subplot(1, 4, 4)\n",
    "show_image(random_resize_crop_transformer(img), \"Random resizing + croping\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the transformations above are random, so if you run the cell multiple times, you will see different results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize some training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take one batch from the train loader\n",
    "data, labels = next(iter(train_loader))\n",
    "data, labels = data[0:5], labels[0:5]\n",
    "\n",
    "# Plot the images\n",
    "fig = plt.figure(figsize=(16, 9))\n",
    "for i in range(0, 5):\n",
    "    fig.add_subplot(1, 5, i + 1)\n",
    "    plt.imshow(data[i].permute(1, 2, 0))\n",
    "    plt.xlabel(class_names[labels[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Loading a pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of available pre-trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torchvision` includes many pre-trained models. Let's get a list and have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in dir(torchvision.models):\n",
    "    if model.startswith(\"_\"): continue  # Skip private properties\n",
    "    print(f\"- {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will use the ResNet-18 architecture:\n",
    "![ResNet-picture](./figures/resnet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's very simple to create a module that has this model with its weights pre-trained for ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = torchvision.models.resnet18(pretrained=True)\n",
    "model_ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A closer look at the ResNet-18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last fully connected layer has a 1000 output neurons. It has been trained on ImageNet task which has 1000 image classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft.fc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to perform binary classification (alien/predator). Therefore, we have to replace the last fully-connected layer to suit our needs (two output units)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft.fc = nn.Linear(in_features=512, out_features=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft.fc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Training just the last layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now the architecture contains two output units, we can therefore use it to perform binary classification.\n",
    "\n",
    "The *train* and _accuracy_ function are almost identical to the functions we used when traininig the CNN. This again nicely demonstrates the modularity of PyTorch and its simple interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import colorama\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    device,\n",
    "    num_epochs=3,\n",
    "    learning_rate=0.1,\n",
    "    decay_learning_rate=False,\n",
    "):\n",
    "    # Some models behave differently in training and testing mode (Dropout, BatchNorm)\n",
    "    # so it is good practice to specify which behavior you want.\n",
    "    model.train()\n",
    "\n",
    "    # We will use the Adam with Cross Entropy loss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    if decay_learning_rate:\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, 0.85)\n",
    "\n",
    "    # We make multiple passes over the dataset\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"=\" * 40, \"Starting epoch %d\" % (epoch + 1), \"=\" * 40)\n",
    "\n",
    "        if decay_learning_rate:\n",
    "            scheduler.step()\n",
    "\n",
    "        total_epoch_loss = 0.0\n",
    "        # Make one pass in batches\n",
    "        for batch_number, (data, labels) in enumerate(train_loader):\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(data)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_epoch_loss += loss.item()\n",
    "\n",
    "            if batch_number % 5 == 0:\n",
    "                print(\"Batch %d/%d\" % (batch_number, len(train_loader)))\n",
    "\n",
    "        train_acc = accuracy(model, train_loader, device)\n",
    "        test_acc = accuracy(model, test_loader, device)\n",
    "\n",
    "        print(\n",
    "            colorama.Fore.GREEN\n",
    "            + \"\\nEpoch %d/%d, Loss=%.4f, Train-Acc=%d%%, Valid-Acc=%d%%\"\n",
    "            % (\n",
    "                epoch + 1,\n",
    "                num_epochs,\n",
    "                total_epoch_loss / len(train_data),\n",
    "                100 * train_acc,\n",
    "                100 * test_acc,\n",
    "            ),\n",
    "            colorama.Fore.RESET,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, data_loader, device):\n",
    "    model.eval()\n",
    "\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    with torch.no_grad():  # deactivates autograd, reduces memory usage and speeds up computations\n",
    "        for data, labels in data_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "            predictions = torch.argmax(model(data), 1)  # find the class number with the largest output\n",
    "            num_correct += (predictions == labels).sum().item()\n",
    "            num_samples += len(predictions)\n",
    "\n",
    "    return num_correct / num_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze all the layers except the last fully-connected one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model_ft.named_parameters():\n",
    "    if name not in [\"fc.weight\", \"fc.bias\"]:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "train(model_ft, train_loader, test_loader, device, num_epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the model's predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = next(iter(DataLoader(test_data, batch_size=5, shuffle=True)))\n",
    "data, labels = data.to(device), labels\n",
    "predictions = torch.argmax(model_ft(data), 1).cpu()\n",
    "\n",
    "predictions, data = predictions.cpu(), data.cpu()  # put it back on CPU for visualization\n",
    "\n",
    "plt.figure(figsize=(16, 9))\n",
    "for i in range(5):\n",
    "    img = data.squeeze(1)[i]\n",
    "    plt.subplot(1, 5, i + 1)\n",
    "    plt.imshow(img.permute(1, 2, 0))\n",
    "    plt.xlabel(\n",
    "        \"pred. = %s\\n (%s)\"\n",
    "        % (test_data.classes[predictions[i].item()], test_data.classes[labels[i]]),\n",
    "        fontsize=14,\n",
    "    )\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--NAVIGATION-->\n",
    "# < [CNN](5-CNN.ipynb) | Transfer Learning |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
